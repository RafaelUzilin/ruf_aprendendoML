# -*- coding: utf-8 -*-
"""Projeto Final ML.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1eNv3J-cR6qdAHTGQh449SUmzmLR3ABFk
"""

import pandas as pd
import numpy as np
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.metrics import f1_score
from sklearn.ensemble import RandomForestClassifier

train = pd.read_csv('train.csv')

train.shape

train.head()

"""Separar dados entre X e y"""

X_train = train.iloc[:,:-1]
y_train = train['income']

X_train

"""Verificação se não há valores nulos em alguma coluna"""

X_train.isnull().sum()

"""Verificar se existem itens com erro de digitação nas"""

cols = X_train.columns[:-1]

print(cols)

for i in cols:
  print(i)
  agrupado = X_train.groupby([i]).size()
  print(agrupado)
  print('')

"""Os dados de testes também possui linhas com "?" então não é necessário tratar

Label encoder e normalização dos dados agora
"""

standard = StandardScaler()
numerical = X_train.select_dtypes(include=['int64','float64']).columns

numerical

X_train[numerical] = standard.fit_transform(X_train[numerical])
X_train

labelencoder = LabelEncoder()
categorical = X_train.select_dtypes(include='object').columns
print(categorical)

for col in categorical:
  X_train[col] = labelencoder.fit_transform(X_train[col])

X_train

"""Label encoder para Y_train também"""

y_train = labelencoder.fit_transform(y_train)

y_train

modelo = RandomForestClassifier(random_state=1, n_estimators=100, max_depth=8, max_leaf_nodes=8)

#n_estimators quantas arvores aleatorias ele vai induzir, por padrão é 100
modelo.fit(X_train, y_train)

data_teste = pd.read_csv('test.csv')

X_teste = data_teste.iloc[:,:-1]
y_teste = data_teste['income']

numerical_teste = X_teste.select_dtypes(include=['int64','float64']).columns

X_teste[numerical_teste] = standard.fit_transform(X_teste[numerical_teste])

X_teste

previsoes = modelo.predict(X_teste)
f1 = f1_score(y_teste, previsoes, average='weighted')

print(f1)

"""=================="""